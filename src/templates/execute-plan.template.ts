import { PromptTemplate } from './prompt-template';
import { getExecuteMetadataSchemaString } from '../schemas/execute-metadata.schema';

/**
 * Execute plan template for executing codebase changes based on a plan
 * This template is used to execute the plan generated by the plan command
 * 
 * Based on the Cursor execute command structure
 */
export function getExecutePlanTemplate(): PromptTemplate {
  const metadataSchema = getExecuteMetadataSchemaString();
  
  return {
    template: `You are executing changes for the codebase (frontend, backend, or both as required).

The requirements are defined by:
1. The referenced plan file at \`{{planPath}}\`
2. The original requirements at \`{{requirementsPath}}\`

Follow instruction precedence (highest to lowest):

1. This command's rules  
2. Agent rules (e.g., \`cursorrules/\`, \`agents.md\`, etc.)  
3. The referenced plan and original requirements

**Important:** If any requirement or plan step is incomplete/ambiguous, do your best to proceed. Make reasonable assumptions and continue execution. The tool is designed to be iterative and will handle refinement in subsequent passes.

---

## Scope Detection

From the plan and requirements, determine scope:

- If clearly frontend-only → limit changes to frontend
- If clearly backend-only → limit changes to backend
- If cross-cutting → implement in both with clear separation

---

## Execution Mode

**Plan-driven execution:**
- Treat the plan file as step-by-step source of truth
- Execute steps in order
- Do not add new scope unless required by the plan or repo rules
- If prior execution progress is evident (e.g., checklist items already completed), continue from the first incomplete step rather than redoing completed steps

---

## Completion Rule

Your goal is to complete ALL relevant plan steps/requirements for this run.

- Do NOT stop just because some tests are still failing after the first attempt
- As long as remaining failures are clearly related to the current step and can be fixed within scope using minimal, safe changes, KEEP ITERATING on that step until the tests pass
- Only stop early if you hit a HARD STOP condition

---

## Hard Blockers

**Hard blockers are RARE** and mean the agent absolutely cannot continue without user input. They break the automatic iterative flow.

Examples of hard blockers:
- Docker service is not running but the plan requires docker commands
- Missing critical credentials that cannot be inferred or worked around
- Required external service is completely unavailable and no workaround exists
- System-level permissions are missing and cannot be obtained programmatically

**NOT hard blockers:**
- Ambiguous requirements (make reasonable assumptions and proceed)
- Test failures (iterate until they pass)
- Missing dependencies (install them)
- Configuration issues (fix them)

If you encounter a hard blocker:
- Add it to the \`hardBlockers\` array in \`execute-metadata.json\` with a clear description and reason
- Continue with other steps if possible, or stop if the blocker prevents all progress
- Document the blocker clearly in the execution summary

---

## Execution Guidelines

As you work:

- Make small, safe, scoped changes
- After each meaningful change, run appropriate lint and focused tests for the affected area (use repo-standard commands)
- Update/add tests to maintain coverage standards and prevent regressions
- Maintain architecture/layering, security, structured errors, and i18n/a11y/routing conventions per repo rules where applicable

---

## Test Strategy

Use a tiered strategy (be conservative with full-suite runs):

1. **Targeted tests** (single file or smallest relevant group)
2. **Focused suite / domain-level tests** (only if clearly needed)
3. **Full suite** (only if explicitly required by the requirements/plan, or strictly necessary due to broad, cross-cutting changes)

Prefer the smallest scope that can meaningfully validate the changes just made.
If a full-suite run is not explicitly required, you may recommend it in "Follow-ups" but MUST NOT run it automatically.

---

## Plan Reference

**Plan file location:** \`{{planPath}}\`

Read and follow the plan file carefully. Execute each step in order, checking off completed items as you go.

**Original requirements:** \`{{requirementsPath}}\`

Refer to the original requirements for context and to ensure alignment with the original intent.

---

## Output Requirements

You MUST output **two files** in \`{{outputDirectory}}\`:

1. **Execution Summary Markdown:** \`{{outputDirectory}}/execution-summary-{{executionIteration}}.md\`
   - This file MUST contain the execution summary with the exact structure below
   - **Note:** This is execution iteration {{executionIteration}} - use this filename for full history tracking

2. **Execute Metadata JSON:** \`{{outputDirectory}}/execute-metadata.json\`
   - This file MUST conform to the exact schema provided below
   - Set \`hasFollowUps\` to \`true\` if there are any follow-ups in the execution summary (even if just one)
   - Add any hard blockers to the \`hardBlockers\` array (should be rare)
   - **CRITICAL:** Follow-ups in the execution summary must be detailed, specific, and **actionable by the agent** - they will be executed programmatically in iterative runs

**Execution Summary Structure** (for \`execution-summary.md\`):

\`\`\`markdown
# Execution Summary
- Mode: <plan-driven|requirements-driven>
- Scope: <frontend|backend|both>

# Checklist
- [ ] Requirement/Step 1: <what changed + file refs>
- [ ] Requirement/Step 2: <...>

# Work Accomplished
- <Summary of what was accomplished in this execution>
- <List key changes, files created/modified, features implemented>
- <Include metrics if relevant: tests added, lines changed, etc.>
- <Be specific and comprehensive - this summary will be used to generate a final report>

# Tests & Lint Run
- <commands or scripts used>

# Deviations
- <None> OR
- <what changed from plan/requirements and why>

# Follow-ups
- <detailed list of follow-up items, each with sufficient context>
\`\`\`

**Important about Follow-ups:**
- Follow-ups are items that need attention but don't prevent execution from continuing
- **CRITICAL: Follow-ups MUST be actionable by the agent** - they will be executed programmatically in iterative runs
- Each follow-up MUST be detailed and specific - include context, file references, and what needs to be done
- Follow-ups will be used for iterative execution, so they must contain enough information for the agent to execute them programmatically

**What makes a good follow-up (agent-actionable):**
- Running tests, linting, or other automated checks: "Run full test suite to verify no regressions were introduced"
- Code changes the agent can make: "Add rate limiting to authentication middleware in \`src/middleware/auth.ts\`"
- Documentation updates: "Update API documentation in \`docs/api.md\` to reflect new endpoint at \`/api/v2/users\`"
- Automated verification: "Run accessibility audit using \`npm run a11y:check\`"
- Dependency updates: "Update \`package.json\` dependencies to latest compatible versions"

**What does NOT make a good follow-up (requires manual human intervention):**
- Manual verification across devices: "Verify UI changes on iPhone, iPad, and Android devices"
- Manual testing that can't be automated: "Manually test the checkout flow with a real credit card"
- Human review/approval: "Have a team member review the security implications"
- Manual deployment steps: "Deploy to staging and manually verify"
- Things requiring human judgment without automated tooling: "Check if the design looks good on mobile"

**Rule:** If a follow-up requires manual human intervention and there's no automated tooling to support it, it should NOT be included as a follow-up. Only include follow-ups that the agent can execute programmatically.

- If there are no follow-ups, use "- None" or leave the section empty

**Execute Metadata JSON Schema:**

\`\`\`json
${metadataSchema}
\`\`\`

**Important:**
- The \`hasFollowUps\` boolean indicates whether the Follow-ups section in the execution summary has any items (set to \`true\` if there are any follow-ups listed, even if just one)
- Hard blockers should be rare - only include them when execution absolutely cannot continue without user intervention
- Each hard blocker must have a clear \`description\` and \`reason\` explaining why it prevents continuation`,
    description: 'Template for executing codebase changes based on a plan',
    requiredVariables: ['planPath', 'requirementsPath', 'outputDirectory', 'executionIteration'],
  };
}
